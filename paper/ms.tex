\documentclass[article]{jss}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Brendon J. Brewer\\The University of Auckland\And 
        Daniel Foreman-Mackey\\University of Washington}
\title{\pkg{DNest4}: Diffusive Nested Sampling in
\proglang{C++} and \proglang{Python}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Brendon J. Brewer, Daniel Foreman-Mackey} %% comma-separated
\Plaintitle{DNest4: An implementation of Diffusive Nested Sampling in
C++11 and Python} %% without formatting
\Shorttitle{\pkg{DNest4}: Diffusive Nested Sampling} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{In probabilistic (Bayesian) inferences, we typically want to compute
properties of the posterior distribution, describing knowledge of
unknown quantities in the context of a particular dataset and the assumed
prior information. The marginal likelihood (also known as the ``evidence'')
is a key quantity in Bayesian model averaging. The Diffusive Nested Sampling algorithm, a variant of Nested Sampling, is a powerful tool for generating
posterior samples and estimating marginal likelihoods. It is effective at
solving complex problems including many where the posterior distribution is
multimodal or has strong dependencies between variables. The key requirement
is that the likelihood function be fast to evaluate.
\pkg{DNest4} is a multi-threaded implementation of this algorithm in
modern \proglang{C++} (specifically, \proglang{C++11}),
along with associated utilities including: i)
a \pkg{Python} package allowing basic use without \proglang{C++} coding;
ii) \pkg{RJObject}, a class template for finite mixture models;
iii) Experimental support for models implemented in \proglang{Julia};
iv) An experimental tool \proglang{Python} tool to
generate \pkg{C++} code from a model specification.
In this paper we demonstrate \pkg{DNest4} usage through examples including
simple Bayesian data analysis, finite mixture models, and Approximate
Bayesian Computation (ABC).
}

\Keywords{bayesian inference, markov chain monte carlo,
metropolis algorithm, bayesian computation, nested sampling, \proglang{c++11},
\proglang{python}}
\Plainkeywords{bayesian inference, markov chain monte carlo,
metropolis algorithm, bayesian computation, nested sampling, c++11, python} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Brendon J. Brewer\\
  Department of Statistics\\
  The University of Auckland\\
  Private Bag 92019\\
  Auckland, 1142\\
  New Zealand\\
  E-mail: \email{bj.brewer@auckland.ac.nz}\\
  URL: \url{https://www.stat.auckland.ac.nz/~brewer/}
}

\newcommand{\params}{\theta}
\newcommand{\data}{D}
\newcommand{\dobs}{D_{\rm obs}}

\begin{document}
\maketitle

% Need this after the abstract
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\section{Bayesian computation and Nested Sampling}


\subsection{Markov Chain Monte Carlo}
In the Metropolis algorithm, the acceptance probability $\alpha$
is given by
\begin{align}
\alpha &= \min\left(1,
\frac{q(\params'|\params)}{q(\params | \params')}
\frac{\pi(\params')}{\pi(\params)}\frac{L(\params')}{L(\params)}
\right)
\end{align}
where $q(\theta' | \theta)$ is the proposal distribution used to generate
a new position $\theta'$ from the current position $\theta$. Often,
$q$ is {\em symmetric} so the $q$ terms cancel in the acceptance probability.


%\subsection{Diffusive Nested Sampling}



%\section{Basic usage of DNest4}

%\section{Advanced usage}

%\subsection{Multithreading}

%\section{Implementing models}

%%\subsection{generate}

%\section{Python package}

%% Assign to DFM

%\section{Examples}
% Reasonably simple examples

The member function used to generate straight line parameters from the
prior is:

\begin{CodeChunk}
\begin{CodeInput}
void StraightLine::from_prior(RNG& rng)
{
   // Naive diffuse prior
   m = 1E3*rng.randn();
   b = 1E3*rng.randn();

   // Log-uniform prior
   sigma = exp(-10.0 + 20.0*rng.rand());

   // Compute the model line
   calculate_mu();
}
\end{CodeInput}
\end{CodeChunk}


\section{Approximate Bayesian Computation}
Nested Sampling can be used to solve ``Approximate Bayesian Computation''
problems elegantly and efficiently.

Approximate Bayesian Computation (ABC), also known as likelihood-free inference,
is a set of Monte Carlo techniques for approximating a posterior distribution
without having to evaluate the likelihood function. Instead, the ability to
generate simulated data sets from the sampling distribution is required.
Since a sampling distribution must be specified, it is not the assumption of
a specific likelihood that is avoided (indeed, a sampling distribution and
a data set imply a particular likelihood function), rather that we cannot
evaluate it cheaply as a function of $\params$ and $\data$.

All Bayesian updating conditions on the truth of a proposition. We sometimes
speak and use notation as if we're conditioning on the value of a variable, for
example by writing the posterior distribution as $p(\params | \data)$. However,
this is shorthand for $p(\params | \data = \dobs)$. In the prior state of
knowledge, the proposition $(\data = \dobs)$ could have been either true or
false, but in the posterior state of knowledge it is definitely true.
In the case of ``continuous data'' (really a continuous {\it space of
possibilities for the data before we learned it}) we're conditioning on a
proposition like $(\data \in [\dobs, \dobs+\epsilon])$ and implicitly
taking the limit $\epsilon \to 0$.

\begin{table}[ht!]
\centering
\small
\begin{tabular}{lll}
\hline
Bayesian computation		&		Statistical mechanics		&		ABC\\
\hline
Parameter space	$\Theta$	&		Phase space	$\Omega$ or configuration space $\Gamma$ 			& (Parameter$\times$Data) space $\Theta \times \mathcal{D}$\\		
Marginal likelihood / evidence	&	Partition function at $T=1$	&\\


\end{tabular}
\caption{\it The relationship between standard Bayesian computation, statistical
mechanics, and ABC. In each case Monte Carlo methods are used to calculate
integrals over a space. In standard Bayesian computation, it is the parameter
space of a model that is usually of interest, whereas in ABC it is the space
of possible parameter values {\it and} data sets.
\label{tab:relation}}
\end{table}


\begin{figure}[ht!]
\centering
\includegraphics[scale=0.5]{figures/joint.pdf}
\caption{\it The joint prior for parameters $\params$ and data $\data$.
\label{fig:joint}}
\end{figure}

The main challenges associated with ABC are:
\begin{enumerate}
\item The choice of summary statistics
\item The choice of distance function
\item How to choose the value of $\epsilon$, or how to change it as a run
progresses
\item How to achieve good mixing.
\end{enumerate}
Challenges 1 and 2 are Bayesian in nature, i.e. they relate to the very
definition of the posterior distribution and are well defined independent of
the fact that we are going to use a Monte Carlo method to compute the
results. On the other hand, challenges 3 and 4 are about the computational implementation.

\section{Parameterising the data space}
In standard MCMC-based ABC, proposal moves are made by first proposing a
change to the parameters (from $\params$ to $\params'$), and then generating
a mock dataset $\data'$ from $p(\data | \params')$. As a Metropolis proposal
for exploring the product space $\Theta \times \mathcal{D}$ this is a very
aggressive choice, since the proposed value of the
dataset $\params'$ is likely to be very different from the previous one.
One of the most elementary pieces of advice we receive when learning the
Metropolis algorithm is to be careful about how ambitious our proposal
distributions are, since tiny and huge proposal moves are both very
inefficient. The performance of MCMC-based ABC can therefore be improved by
alternative choices of proposal distribution.

In some cases, we might be able to find efficient proposal distributions for exploring $\Theta \times \mathcal{D}$ by having insight into the particular
problem we're working on. However, there are also techniques that are
completely general in that they should work on any ABC-type problem.

Consider the process of generating a mock dataset. Whatever else is involved,
this process will involve calls to an underlying random number generator.
Perhaps a function {\tt rand()}, generating variables from a Uniform(0, 1)
distribution, is called $n$ times in the course of generating a dataset.
Usually, the simulation process has the property that, if $\theta'$ is
close to $\theta$, then the same sequence of random numbers returned by
{\tt rand()} would produce a dataset $\data'$ that is very similar to $\data$.
This suggests an alternative parameterisation of
the product space $\Theta\times\mathcal{D}$: instead of using the one that
naturally describes the data, we can use a coordinate system defined by
$\theta$ together with $\{u_1, ..., u_n\}$, the $n$ uniform random numbers
used to construct a mock dataset.

In this alternative coordinate system, a proposed move
to change $\params$ while keeping
$\{u_i\}$ fixed results in a proposed new dataset that is similar to the
previous one, assuming the change to $\params$ is small. The standard procedure
of generating an entirely new mock dataset each step is now viewed as a
proposal where $\params$ is moved {\it and all of the $u_i$ coordinates are
re-generated from their iid uniform prior}.

\section*{Acknowledgements}
It is a pleasure to thank Ewan Cameron (Oxford), Dan
Foreman-Mackey (Washington), David Hogg (NYU), Daniela Huppenkothen (NYU),
Scott Sisson (UNSW), and Yanan Fan (NYU) for valuable discussions.

\begin{thebibliography}{99}
\bibitem[\protect\citeauthoryear{Brewer, P{\'a}rtay,
\& Cs{\'a}nyi}{2011}]{dnest} Brewer B.~J., P{\'a}rtay L.~B., Cs{\'a}nyi G., 2011,
Statistics and Computing, 21, 4, 649-656. arXiv:0912.2380

\bibitem[Caticha(2008)]{caticha} Caticha, A.\ 2008.\ Lectures 
on Probability, Entropy, and Statistical Physics.\ ArXiv e-prints 
arXiv:0808.0012.

\end{thebibliography}

\end{document}

